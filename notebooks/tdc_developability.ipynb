{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antibody Developability Prediction with E(N) GNNs\n",
    "\n",
    "Here we showcase Graphein's ability to generate a structural dataset from only PDB identifiers and labels. We use an [antibody developability dataset](https://tdcommons.ai/single_pred_tasks/develop/) from [TDC](https://tdcommons.ai/) and train an [E(N) GNN](https://arxiv.org/abs/2102.09844) on antibody structure graphs.\n",
    "\n",
    "**Dataset Description**: Antibody data from Chen et al [1], where they process from the SAbDab [2]. From an initial dataset of 3816 antibodies, they retained 2426 antibodies that satisfy the following criteria:\n",
    "\n",
    "1. have both sequence (FASTA) and Protein Data Bank (PDB) structure files,\n",
    "2. contain both a heavy chain and a light chain, and\n",
    "3. have crystal structures with resolution < 3 Å. The DI label is derived from BIOVIA's pipelines [3].\n",
    "\n",
    "**Task Description**: Binary classification. Given the antibody's heavy chain and light chain sequence, predict its developability. The input X is a list of two sequences where the first is the heavy chain and the second light chain.\n",
    "\n",
    "**Dataset Statistics**: 2,409 antibodies.\n",
    "\n",
    "**Dataset Split**: Random Split\n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] Chen, Xingyao, et al. “Predicting antibody developability from sequence using machine learning.” bioRxiv (2020).\n",
    "\n",
    "[2] Dunbar, James, et al. “SAbDab: the structural antibody database.” Nucleic acids research 42.D1 (2014): D1140-D1146.\n",
    "\n",
    "[3] Biovia, Dassault Systèmes. “BIOVIA pipeline pilot.” Dassault Systèmes: San Diego, BW, Release (2017).\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/a-r-j/graphein/blob/master/notebooks/tdc_developability.ipynb) [![GitHub](https://img.shields.io/badge/-View%20on%20GitHub-181717?logo=github&logoColor=ffffff)](https://github.com/a-r-j/graphein/blob/master/notebooks/tdc_developability.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements if necessary\n",
    "# !pip install graphein\n",
    "# !pip install PyTDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "import torch\n",
    "from typing import Dict\n",
    "\n",
    "from tdc.single_pred import Develop\n",
    "\n",
    "import graphein.protein as gp\n",
    "from graphein.ml.conversion import GraphFormatConvertor\n",
    "from graphein.ml import InMemoryProteinGraphDataset, ProteinGraphDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from TDC\n",
    "As this dataset is non-redundant we can use a random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "# Load data from TDC and split\n",
    "data = Develop(name = 'SAbDab_Chen')\n",
    "split = data.get_split()\n",
    "split[\"train\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Obsolete Structures\n",
    "\n",
    "Sometimes PDB entries are made obsolete as improved structures are made available. This is problematic as the old structures are not available for download. For instance, see [1OM3](https://www.rcsb.org/structure/removed/1OM3) which has been replaced by [6N32](https://www.rcsb.org/structure/6N32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "# Check for obsolete structures\n",
    "from graphein.protein.utils import get_obsolete_mapping\n",
    "\n",
    "obs = get_obsolete_mapping()\n",
    "\n",
    "train_obs = [t for t in split[\"train\"][\"Antibody_ID\"] if t in obs.keys()]\n",
    "valid_obs = [t for t in split[\"valid\"][\"Antibody_ID\"] if t in obs.keys()]\n",
    "test_obs = [t for t in split[\"test\"][\"Antibody_ID\"] if t in obs.keys()]\n",
    "\n",
    "print(train_obs)\n",
    "print(valid_obs)\n",
    "print(test_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "# If you want, you can get the PDB IDs of the new structure that replaces the obsolete entry\n",
    "print(\"Replacement PDBs: \", [obs[t] for t  in train_obs])\n",
    "\n",
    "# However, in this instance we will simply remove the obsolete entries from the train and test sets.\n",
    "split[\"train\"] = split[\"train\"].loc[~split[\"train\"][\"Antibody_ID\"].isin(train_obs)]\n",
    "split[\"test\"] = split[\"test\"].loc[~split[\"test\"][\"Antibody_ID\"].isin(test_obs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Labels\n",
    "\n",
    "We convert the labels to tensors and map them to their corresponding PDB ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "# Convert labels to tensors\n",
    "def get_label_map(split_name: str) -> Dict[str, torch.Tensor]:\n",
    "    return dict(zip(split[split_name].Antibody_ID, split[split_name].Y.apply(torch.tensor)))\n",
    "\n",
    "train_labels = get_label_map(\"train\")\n",
    "valid_labels = get_label_map(\"valid\")\n",
    "test_labels = get_label_map(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Graphs and Dataloaders with Graphein\n",
    "\n",
    "### Configuration and Conversion\n",
    "\n",
    "First, we define a [configuration object](https://graphein.ai/modules/graphein.protein.html#graphein.protein.config.ProteinGraphConfig) which governs the graph construction. Here we use a simple graph for illustrative purposes, using only a one-hot encoding of the amino acid type, the node coordinates and edges based on spatial contacts within 6 angstroms.\n",
    "\n",
    "Secondly, we define a [convertor](https://graphein.ai/modules/graphein.ml.html#conversion) which converts from NetworkX graphs to Pytorch Geometric `Data` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "from functools import partial\n",
    "\n",
    "graphein_config = gp.ProteinGraphConfig(\n",
    "    node_metadata_functions=[gp.amino_acid_one_hot],\n",
    "    edge_construction_functions=[partial(gp.add_distance_threshold, threshold=6, long_interaction_threshold=0)])\n",
    "\n",
    "convertor = GraphFormatConvertor(src_format=\"nx\", dst_format=\"pyg\", columns=[\"coords\", \"edge_index\", \"amino_acid_one_hot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "# Quickly visualise a protein to see what the config looks like in practice\n",
    "gp.plotly_protein_structure_graph(gp.construct_graph(pdb_code=\"1lds\", config=graphein_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Datasets\n",
    "\n",
    "Next, we create the actual structural datasets. This takes care of downloading the raw structural data, pre-processing, graph conversion and caches the dataset for easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "train_ds = InMemoryProteinGraphDataset(\n",
    "    root=\"./data/\",\n",
    "    name=\"train\",\n",
    "    pdb_codes=split[\"train\"][\"Antibody_ID\"],\n",
    "    graph_label_map=train_labels,\n",
    "    graphein_config=graphein_config,\n",
    "    graph_format_convertor=convertor\n",
    "    )\n",
    "\n",
    "valid_ds = InMemoryProteinGraphDataset(\n",
    "    root=\"./data/\",\n",
    "    name=\"valid\",\n",
    "    pdb_codes=split[\"valid\"][\"Antibody_ID\"],\n",
    "    graph_label_map=valid_labels,\n",
    "    graphein_config=graphein_config,\n",
    "    graph_format_convertor=convertor\n",
    "    )\n",
    "\n",
    "test_ds = InMemoryProteinGraphDataset(\n",
    "    root=\"./data/\",\n",
    "    name=\"test\",\n",
    "    pdb_codes=split[\"test\"][\"Antibody_ID\"],\n",
    "    graph_label_map=test_labels,\n",
    "    graphein_config=graphein_config,\n",
    "    graph_format_convertor=convertor\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we wrap the datasets to create dataloaders for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=False, drop_last=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=16, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=16, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "# Inspect a batch\n",
    "for b in valid_loader:\n",
    "    print(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Here we lift the implementation from the github repository accompanying the paper https://github.com/vgsatorras/egnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "\"\"\"EGNN Implementation from Satorras et al. https://github.com/vgsatorras/egnn\"\"\"\n",
    "\n",
    "class E_GCL(nn.Module):\n",
    "    \"\"\"\n",
    "    E(n) Equivariant Convolutional Layer\n",
    "    re\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_nf, output_nf, hidden_nf, edges_in_d=0, act_fn=nn.SiLU(), residual=True, attention=False, normalize=False, coords_agg='mean', tanh=False):\n",
    "        super(E_GCL, self).__init__()\n",
    "        input_edge = input_nf * 2\n",
    "        self.residual = residual\n",
    "        self.attention = attention\n",
    "        self.normalize = normalize\n",
    "        self.coords_agg = coords_agg\n",
    "        self.tanh = tanh\n",
    "        self.epsilon = 1e-8\n",
    "        edge_coords_nf = 1\n",
    "\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(input_edge + edge_coords_nf + edges_in_d, hidden_nf),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden_nf, hidden_nf),\n",
    "            act_fn)\n",
    "\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_nf + input_nf, hidden_nf),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden_nf, output_nf))\n",
    "\n",
    "        layer = nn.Linear(hidden_nf, 1, bias=False)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "\n",
    "        coord_mlp = [nn.Linear(hidden_nf, hidden_nf)]\n",
    "        coord_mlp.append(act_fn)\n",
    "        coord_mlp.append(layer)\n",
    "        if self.tanh:\n",
    "            coord_mlp.append(nn.Tanh())\n",
    "        self.coord_mlp = nn.Sequential(*coord_mlp)\n",
    "\n",
    "        if self.attention:\n",
    "            self.att_mlp = nn.Sequential(\n",
    "                nn.Linear(hidden_nf, 1),\n",
    "                nn.Sigmoid())\n",
    "\n",
    "    def edge_model(self, source, target, radial, edge_attr):\n",
    "        if edge_attr is None:  # Unused.\n",
    "            out = torch.cat([source, target, radial], dim=1)\n",
    "        else:\n",
    "            out = torch.cat([source, target, radial, edge_attr], dim=1)\n",
    "        out = self.edge_mlp(out)\n",
    "        if self.attention:\n",
    "            att_val = self.att_mlp(out)\n",
    "            out = out * att_val\n",
    "        return out\n",
    "\n",
    "    def node_model(self, x, edge_index, edge_attr, node_attr):\n",
    "        row, col = edge_index\n",
    "        agg = unsorted_segment_sum(edge_attr, row, num_segments=x.size(0))\n",
    "        if node_attr is not None:\n",
    "            agg = torch.cat([x, agg, node_attr], dim=1)\n",
    "        else:\n",
    "            agg = torch.cat([x, agg], dim=1)\n",
    "        out = self.node_mlp(agg)\n",
    "        if self.residual:\n",
    "            out = x + out\n",
    "        return out, agg\n",
    "\n",
    "    def coord_model(self, coord, edge_index, coord_diff, edge_feat):\n",
    "        row, col = edge_index\n",
    "        trans = coord_diff * self.coord_mlp(edge_feat)\n",
    "        if self.coords_agg == 'sum':\n",
    "            agg = unsorted_segment_sum(trans, row, num_segments=coord.size(0))\n",
    "        elif self.coords_agg == 'mean':\n",
    "            agg = unsorted_segment_mean(trans, row, num_segments=coord.size(0))\n",
    "        else:\n",
    "            raise Exception('Wrong coords_agg parameter' % self.coords_agg)\n",
    "        coord += agg\n",
    "        return coord\n",
    "\n",
    "    def coord2radial(self, edge_index, coord):\n",
    "        row, col = edge_index\n",
    "        coord_diff = coord[row] - coord[col]\n",
    "        radial = torch.sum(coord_diff**2, 1).unsqueeze(1)\n",
    "\n",
    "        if self.normalize:\n",
    "            norm = torch.sqrt(radial).detach() + self.epsilon\n",
    "            coord_diff = coord_diff / norm\n",
    "\n",
    "        return radial, coord_diff\n",
    "\n",
    "    def forward(self, h, edge_index, coord, edge_attr=None, node_attr=None):\n",
    "        row, col = edge_index\n",
    "        radial, coord_diff = self.coord2radial(edge_index, coord)\n",
    "\n",
    "        edge_feat = self.edge_model(h[row], h[col], radial, edge_attr)\n",
    "        coord = self.coord_model(coord, edge_index, coord_diff, edge_feat)\n",
    "        h, agg = self.node_model(h, edge_index, edge_feat, node_attr)\n",
    "\n",
    "        return h, coord, edge_attr\n",
    "\n",
    "class EGNN(nn.Module):\n",
    "    def __init__(self, in_node_nf, hidden_nf, out_node_nf, in_edge_nf=0, device='cpu', act_fn=nn.SiLU(), n_layers=4, residual=True, attention=False, normalize=False, tanh=False):\n",
    "        '''\n",
    "        :param in_node_nf: Number of features for 'h' at the input\n",
    "        :param hidden_nf: Number of hidden features\n",
    "        :param out_node_nf: Number of features for 'h' at the output\n",
    "        :param in_edge_nf: Number of features for the edge features\n",
    "        :param device: Device (e.g. 'cpu', 'cuda:0',...)\n",
    "        :param act_fn: Non-linearity\n",
    "        :param n_layers: Number of layer for the EGNN\n",
    "        :param residual: Use residual connections, we recommend not changing this one\n",
    "        :param attention: Whether using attention or not\n",
    "        :param normalize: Normalizes the coordinates messages such that:\n",
    "                    instead of: x^{l+1}_i = x^{l}_i + Σ(x_i - x_j)phi_x(m_ij)\n",
    "                    we get:     x^{l+1}_i = x^{l}_i + Σ(x_i - x_j)phi_x(m_ij)/||x_i - x_j||\n",
    "                    We noticed it may help in the stability or generalization in some future works.\n",
    "                    We didn't use it in our paper.\n",
    "        :param tanh: Sets a tanh activation function at the output of phi_x(m_ij). I.e. it bounds the output of\n",
    "                        phi_x(m_ij) which definitely improves in stability but it may decrease in accuracy.\n",
    "                        We didn't use it in our paper.\n",
    "        '''\n",
    "\n",
    "        super(EGNN, self).__init__()\n",
    "        self.hidden_nf = hidden_nf\n",
    "        self.device = device\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding_in = nn.Linear(in_node_nf, self.hidden_nf)\n",
    "        self.embedding_out = nn.Linear(self.hidden_nf, out_node_nf)\n",
    "        for i in range(n_layers):\n",
    "            self.add_module(\"gcl_%d\" % i, E_GCL(self.hidden_nf, self.hidden_nf, self.hidden_nf, edges_in_d=in_edge_nf,\n",
    "                                                act_fn=act_fn, residual=residual, attention=attention,\n",
    "                                                normalize=normalize, tanh=tanh))\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, h, x, edges, edge_attr):\n",
    "        h = self.embedding_in(h)\n",
    "        for i in range(self.n_layers):\n",
    "            h, x, _ = self._modules[\"gcl_%d\" % i](h, edges, x, edge_attr=edge_attr)\n",
    "        h = self.embedding_out(h)\n",
    "        return h, x\n",
    "\n",
    "\n",
    "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
    "    result_shape = (num_segments, data.size(1))\n",
    "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
    "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
    "    result.scatter_add_(0, segment_ids, data)\n",
    "    return result\n",
    "\n",
    "\n",
    "def unsorted_segment_mean(data, segment_ids, num_segments):\n",
    "    result_shape = (num_segments, data.size(1))\n",
    "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
    "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
    "    count = data.new_full(result_shape, 0)\n",
    "    result.scatter_add_(0, segment_ids, data)\n",
    "    count.scatter_add_(0, segment_ids, torch.ones_like(data))\n",
    "    return result / count.clamp(min=1)\n",
    "\n",
    "\n",
    "def get_edges(n_nodes):\n",
    "    rows, cols = [], []\n",
    "    for i, j in itertools.product(range(n_nodes), range(n_nodes)):\n",
    "        if i != j:\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "\n",
    "    return [rows, cols]\n",
    "\n",
    "\n",
    "def get_edges_batch(n_nodes, batch_size):\n",
    "    edges = get_edges(n_nodes)\n",
    "    edge_attr = torch.ones(len(edges[0]) * batch_size, 1)\n",
    "    edges = [torch.LongTensor(edges[0]), torch.LongTensor(edges[1])]\n",
    "    if batch_size == 1:\n",
    "        return edges, edge_attr\n",
    "    elif batch_size > 1:\n",
    "        rows, cols = [], []\n",
    "        for i in range(batch_size):\n",
    "            rows.append(edges[0] + n_nodes * i)\n",
    "            cols.append(edges[1] + n_nodes * i)\n",
    "        edges = [torch.cat(rows), torch.cat(cols)]\n",
    "    return edges, edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits, mse_loss\n",
    "from torch_geometric.nn import global_add_pool\n",
    "import pytorch_lightning as pl\n",
    "#from torchmetrics import F1Score, Accuracy, AUROC\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "\n",
    "class SimpleEGNN(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = EGNN(\n",
    "            in_node_nf=20,\n",
    "            out_node_nf=32,\n",
    "            in_edge_nf=0,\n",
    "            hidden_nf=32,\n",
    "            n_layers=2,\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "        self.loss = binary_cross_entropy_with_logits\n",
    "\n",
    "    def configure_loss(self, name: str):\n",
    "        \"\"\"Return the loss function based on the config.\"\"\"\n",
    "        return self.loss\n",
    "\n",
    "    # --- Forward pass\n",
    "    def forward(self, x):\n",
    "        x.aa = torch.cat([torch.tensor(a) for a in x.amino_acid_one_hot]).float().cuda()\n",
    "        x.c = torch.cat([torch.tensor(a).squeeze(0) for a in x.coords]).float().cuda()\n",
    "        feats, coords = self.model(\n",
    "            h=x.aa,\n",
    "            x=x.c,\n",
    "            edges=x.edge_index,\n",
    "            edge_attr=None,\n",
    "        )\n",
    "        feats = global_add_pool(feats, x.batch)\n",
    "        return self.decoder(feats)\n",
    "\n",
    "    def training_step(self, batch: Data, batch_idx) -> torch.Tensor:\n",
    "        x = batch\n",
    "        y = batch.graph_y.unsqueeze(1).float()\n",
    "        y_hat = self(x)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        y = batch.graph_y.unsqueeze(1).float()\n",
    "        y_hat = self(batch)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        y = batch.graph_y.unsqueeze(1).float()\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "\n",
    "        y_pred_softmax = torch.log_softmax(y_hat, dim = 1)\n",
    "        y_pred_tags = torch.argmax(y_pred_softmax, dim = 1)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        #return loss\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        return torch.optim.Adam(params=self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "trainer = pl.Trainer(\n",
    "    strategy=None,\n",
    "    gpus=1,\n",
    "    benchmark=True,\n",
    "    deterministic=False,\n",
    "    num_sanity_val_steps=0,\n",
    "    max_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "model = SimpleEGNN()\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but maybe you can do better! :)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ab7f988027852efc1ebacd06db3f130eb65d2a20cb6a366311359132c20a952"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('graphein')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
