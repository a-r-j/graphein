{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Structural Change Prediction Example\n",
    "In this notebook, we use Graphein to preprocess the PSCDB database into graphs. We then perform graph classification on the unbound protein ligand graphs to predict the class of structural rearrangement the protein undergoes upon ligand binding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dgllife.model.model_zoo import GCNPredictor\n",
    "from graphein.construct_graphs import ProteinGraph\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "1.  We load the dataset. The script for parsing the datafrom a webserver to the `structural_rearrangement_data.csv` file is available in `process_data.ipynb` and `make_rearrangement_data.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sets\n",
    "df = pd.read_csv('structural_rearrangement_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We create one-hot encodings of the labels, indicating the rearrangement motion class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels\n",
    "labels = pd.get_dummies(df.motion_type).values.tolist()\n",
    "labels = [torch.Tensor(i) for i in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We split the data into training and testing data, and construct the Graphs using Graphein. \n",
    "\n",
    "Graphein will automatically download the relevant `.PDB` files from the PDB and compute the intramolecular contacts using `GetContacts` if the files are not found in the `pdb_dir` and `contacts_dir` directories.\n",
    "\n",
    "We select the relevant chains in structure from the PDB from the `Free Chains` column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split datasets\n",
    "x_train, x_test, y_train, y_test = train_test_split(df, labels, test_size=0.15)\n",
    "\n",
    "# Initialise Graph Constructor\n",
    "pg = ProteinGraph(granularity='CA', insertions=False, keep_hets=True,\n",
    "                  node_featuriser='meiler', get_contacts_path='/Users/arianjamasb/github/getcontacts',\n",
    "                  pdb_dir='../../examples/pdbs/',\n",
    "                  contacts_dir='../../examples/contacts/',\n",
    "                  exclude_waters=True, covalent_bonds=False, include_ss=True)\n",
    "\n",
    "# Build Graphs\n",
    "train_graphs = [pg.dgl_graph_from_pdb_code(pdb_code=x_train['Free PDB'].iloc[i],\n",
    "                                         chain_selection=list(x_train['Free Chains'].iloc[i])) for i in tqdm(range(len(x_train)))]\n",
    "\n",
    "test_graphs = [pg.dgl_graph_from_pdb_code(pdb_code=x_test['Free PDB'].iloc[i],\n",
    "                                         chain_selection=list(x_test['Free Chains'].iloc[i])) for i in tqdm(range(len(x_test)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs, node_attrs='h')\n",
    "    batched_graph.set_n_initializer(dgl.init.zero_initializer)\n",
    "    batched_graph.set_e_initializer(dgl.init.zero_initializer)\n",
    "    return batched_graph, torch.stack(labels)\n",
    "\n",
    "train_data = list(zip(train_graphs, y_train))\n",
    "test_data = list(zip(test_graphs, y_test))\n",
    "\n",
    "#Create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True,\n",
    "                         collate_fn=collate)\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=True,\n",
    "                         collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Here, we define a simple GCN for graph classification. We then train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feats = train_graphs[1].ndata['h'].shape[1]\n",
    "\n",
    "# Instantiate model\n",
    "gcn_net = GCNPredictor(in_feats=n_feats,\n",
    "                       hidden_feats=[32, 32],\n",
    "                       batchnorm=[True, True],\n",
    "                       dropout=[0, 0],\n",
    "                       classifier_hidden_feats=32,\n",
    "                       n_tasks=7\n",
    "                       )\n",
    "gcn_net.to(device)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gcn_net.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "\n",
    "# Training loop\n",
    "gcn_net.train()\n",
    "epoch_losses = []\n",
    "\n",
    "epoch_f1_scores = [] \n",
    "epoch_precision_scores = []\n",
    "epoch_recall_scores = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    preds = []\n",
    "    labs = []\n",
    "    # Train on batch\n",
    "    for i, (bg, labels) in enumerate(train_loader):\n",
    "        labels = labels.to(device)\n",
    "        graph_feats = bg.ndata.pop('h').to(device)\n",
    "        graph_feats, labels = graph_feats.to(device), labels.to(device)\n",
    "        y_pred = gcn_net(bg, graph_feats)\n",
    "        \n",
    "        preds.append(y_pred.detach().numpy())\n",
    "        labs.append(labels.detach().numpy())\n",
    "\n",
    "        labels = np.argmax(labels, axis=1)\n",
    "        \n",
    "        loss = loss_fn(y_pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        \n",
    "    epoch_loss /= (i + 1)\n",
    "    \n",
    "    preds = np.vstack(preds)\n",
    "    labs = np.vstack(labs)\n",
    "    \n",
    "    # There's some sort of issue going on here with the scoring. All three values are the same\n",
    "    f1 = f1_score(np.argmax(labs, axis=1), np.argmax(preds, axis=1), average='micro')\n",
    "    precision = precision_score(np.argmax(labs, axis=1), np.argmax(preds, axis=1), average='micro')\n",
    "    recall = recall_score(np.argmax(labs, axis=1), np.argmax(preds, axis=1), average='micro')\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"epoch: {epoch}, LOSS: {epoch_loss:.3f}, F1: {f1:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}\")\n",
    "        \n",
    "    epoch_losses.append(epoch_loss)\n",
    "    epoch_f1_scores.append(f1)\n",
    "    epoch_precision_scores.append(precision)\n",
    "    epoch_recall_scores.append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_losses, label=\"Loss\")\n",
    "plt.plot(epoch_f1_scores, label='F1')\n",
    "plt.plot(epoch_precision_scores, label=\"Precision\")\n",
    "plt.plot(epoch_recall_scores, label=\"Recall\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "gcn_net.eval()\n",
    "test_loss = 0\n",
    "\n",
    "preds = []\n",
    "labs = []\n",
    "for i, (bg, labels) in enumerate(test_loader):\n",
    "    labels = labels.to(device)\n",
    "    graph_feats = bg.ndata.pop('h').to(device)\n",
    "    graph_feats, labels = graph_feats.to(device), labels.to(device)\n",
    "    y_pred = gcn_net(bg, graph_feats)\n",
    "\n",
    "    preds.append(y_pred.detach().numpy())\n",
    "    labs.append(labels.detach().numpy())\n",
    "\n",
    "labs = np.vstack(labs)\n",
    "preds = np.vstack(preds)\n",
    "\n",
    "f1 = f1_score(np.argmax(labs, axis=1), np.argmax(preds, axis=1), average='micro')\n",
    "precision = precision_score(np.argmax(labs, axis=1), np.argmax(preds, axis=1), average='micro')\n",
    "recall = recall_score(np.argmax(labs, axis=1), np.argmax(preds, axis=1), average='micro')\n",
    "\n",
    "print(f\"TEST F1: {f1:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Graphein",
   "language": "python",
   "name": "graphein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
