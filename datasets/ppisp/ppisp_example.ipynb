{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "from torch.nn import NLLLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from Bio.PDB import *\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, matthews_corrcoef\n",
    "\n",
    "from graphein.construct_graphs import ProteinGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "Here, we load the dataset provided in DeepPPISP. We first load the node labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DeepPPISP Data\n",
    "with open('all_dset_list.pkl', 'rb') as f:\n",
    "    index = pickle.load(f)\n",
    "\n",
    "with open('dset186_label.pkl', 'rb') as f:\n",
    "    dset186_labels = pickle.load(f)\n",
    "\n",
    "with open('dset164_label.pkl', 'rb') as f:\n",
    "    dset164_labels = pickle.load(f)\n",
    "\n",
    "with open('dset72_label.pkl', 'rb') as f:\n",
    "    dset72_labels = pickle.load(f)\n",
    "\n",
    "labels = dset186_labels + dset164_labels + dset72_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PSSMs\n",
    "with open('dset186_pssm_data.pkl', 'rb') as f:\n",
    "    dset_186_pssms = pickle.load(f)\n",
    "\n",
    "with open('dset164_pssm_data.pkl', 'rb') as f:\n",
    "    dset_164_pssms = pickle.load(f)\n",
    "\n",
    "with open('dset72_pssm_data.pkl', 'rb') as f:\n",
    "    dset_72_pssms = pickle.load(f)\n",
    "\n",
    "pssms = dset_186_pssms + dset_164_pssms + dset_72_pssms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write labels\n",
    "#pickle.dump(labels, open('ppisp_node_labels.p', 'wb'))\n",
    "\n",
    "df = pd.DataFrame(index)\n",
    "df.columns = ['pos_index', 'example_index', 'res_position', 'dataset', 'pdb', 'length']\n",
    "\n",
    "df = df.loc[df['res_position'] == 0]\n",
    "\n",
    "# Get PDB accession and chains\n",
    "df[['pdb_code', 'chains']] = df.pdb.str.split(\"_\", expand=True)\n",
    "df['pdb_code'] = df['pdb_code'].str.lower()\n",
    " \n",
    "    \n",
    "# These columns don't follow the format\n",
    "df.loc[df['dataset'] == 'dset164', 'pdb_code'] = df.copy().loc[df['dataset'] == 'dset164']['pdb'].str.slice(stop=4)\n",
    "df.loc[df['dataset'] == 'dset164', 'chains'] = df.copy().loc[df['dataset'] == 'dset164']['pdb'].str.slice(-1)\n",
    "df['chains'] = df['chains'].fillna('all')\n",
    "# Remove Obsolete structures\n",
    "obsolete = ['3NW0', '3VDO']\n",
    "replacements = ['', '4NQW']\n",
    "df = df.loc[~df['pdb_code'].isin(obsolete)]\n",
    "\n",
    "# Assign training/test status\n",
    "with open('training_list.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open('testing_list.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "df.loc[df['pos_index'].isin(train), 'train'] = 1\n",
    "df.loc[df['pos_index'].isin(test), 'train'] = 0\n",
    "df.reset_index(inplace=True)\n",
    "#Write Dataframe\n",
    "#df.to_csv('deepppisp_clean.csv')\n",
    "df['pdb_code'] = df['pdb_code'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Protein Graph Class\n",
    "pg = ProteinGraph(granularity='CA',\n",
    "                  insertions=False,\n",
    "                  keep_hets=False,\n",
    "                  node_featuriser='meiler',\n",
    "                  get_contacts_path='/home/arj39/Documents/github/getcontacts',\n",
    "                  pdb_dir='ppisp_pdbs/',\n",
    "                  contacts_dir='ppisp_contacts/',\n",
    "                  exclude_waters=True,\n",
    "                  covalent_bonds=False,\n",
    "                  include_ss=True,\n",
    "                  include_ligand=False,\n",
    "                  edge_distance_cutoff=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list = []\n",
    "label_list = []\n",
    "test_indices = []\n",
    "train_indices = []\n",
    "idx_counter = 0\n",
    "for example in tqdm(range(len(labels))):\n",
    "    # Create Protein Graph\n",
    "    try:\n",
    "        # Construct graph using Graphein\n",
    "        g = pg.dgl_graph_from_pdb_code(pdb_code=df['pdb_code'][example],\n",
    "                                       chain_selection=list(df['chains'][example]),\n",
    "                                       edge_construction=['contacts']\n",
    "                                       )\n",
    "        # Create PSSM Feats and label\n",
    "        df_index = df.iloc[example]['example_index']\n",
    "        label = labels[df_index]\n",
    "        pssm = pssms[df_index]\n",
    "                \n",
    "    except:\n",
    "        print(f'Failed on example {example}')\n",
    "        break\n",
    "    \n",
    "    # Ensure node labels match number of nodes. There are a few cases (~5) where this doesn't hold. We skip these.\n",
    "    if g.number_of_nodes() != len(label):\n",
    "        print('label length does not match ', example)\n",
    "        print(g.number_of_nodes())\n",
    "        print(len(label))\n",
    "        continue\n",
    "    if g.number_of_nodes() != len(pssm):\n",
    "        print(g.number_of_nodes())\n",
    "        print(len(pssm))\n",
    "        print('pssm length does not match', example)\n",
    "        continue\n",
    "\n",
    "    # Track training and test indices\n",
    "    if df['train'][example] == 0:\n",
    "        test_indices.append(idx_counter)\n",
    "    if df['train'][example] == 1:\n",
    "        train_indices.append(idx_counter)\n",
    "    idx_counter += 1\n",
    "    \n",
    "    # Concatenate graph features and store graph and labels as 'feats'\n",
    "    g.ndata['feats'] = torch.cat((g.ndata['h'],\n",
    "                               g.ndata['ss'],\n",
    "                               g.ndata['asa'],\n",
    "                               g.ndata['rsa'],\n",
    "                               g.ndata['coords'],\n",
    "                               torch.Tensor(pssm)), dim=1)\n",
    "    graph_list.append(g)\n",
    "    \n",
    "    label = torch.Tensor(label).long()\n",
    "    label_list.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise graph features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graphs = [graph_list[i] for i in test_indices]\n",
    "train_graphs = [graph_list[i] for i in train_indices]\n",
    "\n",
    "print(f\"Train graphs: {len(train_graphs)}\")\n",
    "print(f\"Test graphs: {len(test_graphs)}\")\n",
    "\n",
    "train_labels = [label_list[i] for i in train_indices]\n",
    "test_labels = [label_list[i] for i in test_indices]\n",
    "\n",
    "# Compute feature min/maxes for normalisation\n",
    "train_feats = torch.cat([graph.ndata['feats'] for graph in train_graphs], dim=0)\n",
    "max_feats = torch.max(train_feats, dim=0)[0]\n",
    "min_feats = torch.min(train_feats, dim=0)[0]\n",
    "\n",
    "max_feats[max_feats == 0] = 1\n",
    "# Normalise train and test graph features\n",
    "for g in train_graphs:\n",
    "    g.ndata['feats'] -= min_feats\n",
    "    g.ndata['feats'] /= max_feats\n",
    "    \n",
    "for g in test_graphs:\n",
    "    g.ndata['feats'] -= min_feats\n",
    "    g.ndata['feats'] /= max_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define collate function. This batches the graphs, and concatenates the label tensors, such that each batch is a batched DGL graph and the labels a corresponding binary tensor\n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs, node_attrs='feats')\n",
    "    batched_graph.set_n_initializer(dgl.init.zero_initializer)\n",
    "    batched_graph.set_e_initializer(dgl.init.zero_initializer)\n",
    "    return batched_graph, torch.cat(labels)\n",
    "\n",
    "train_data = list(zip(train_graphs, train_labels))\n",
    "test_data = list(zip(test_graphs, test_labels))\n",
    "\n",
    "#Create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True,\n",
    "                         collate_fn=collate)\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=True,\n",
    "                         collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "Adapted from: https://docs.dgl.ai/en/0.2.x/tutorials/basics/1_first.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the message & reduce function\n",
    "# NOTE: we ignore the GCN's normalization constant c_ij for this tutorial.\n",
    "def gcn_message(edges):\n",
    "    # The argument is a batch of edges.\n",
    "    # This computes a (batch of) message called 'msg' using the source node's feature 'h'.\n",
    "    return {'msg' : edges.src['h']}\n",
    "\n",
    "def gcn_reduce(nodes):\n",
    "    # The argument is a batch of nodes.\n",
    "    # This computes the new 'h' features by summing received 'msg' in each node's mailbox.\n",
    "    return {'h' : torch.sum(nodes.mailbox['msg'], dim=1)}\n",
    "\n",
    "# Define the GCNLayer module\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # g is the graph and the inputs is the input node features\n",
    "        # first set the node features\n",
    "        g.ndata['h'] = inputs\n",
    "        # trigger message passing on all edges\n",
    "        g.send(g.edges(), gcn_message)\n",
    "        # trigger aggregation at all nodes\n",
    "        g.recv(g.nodes(), gcn_reduce)\n",
    "        # get the result node features\n",
    "        h = g.ndata.pop('h')\n",
    "        # perform linear transformation\n",
    "        return self.linear(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gcn1 = GCNLayer(in_feats, hidden_size)\n",
    "        self.gcn2 = GCNLayer(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = self.gcn1(g, inputs)\n",
    "        h = torch.relu(h)\n",
    "        h = self.gcn2(g, h)\n",
    "        return h\n",
    "# The first layer transforms input features of size of 41 to a hidden size of 5.\n",
    "# The second layer transforms the hidden layer and produces output features of\n",
    "# size 2, corresponding to the two classification groups\n",
    "net = GCN(41, 16, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "train_logits = []\n",
    "\n",
    "epochs = 50\n",
    "epoch_losses = []\n",
    "\n",
    "epoch_f1_scores = [] \n",
    "epoch_precision_scores = []\n",
    "epoch_recall_scores = []\n",
    "\n",
    "loss_fn = nn.NLLLoss(weight=torch.Tensor([1, 5.84]))\n",
    "\n",
    "# Training loop\n",
    "net.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    epoch_logits = []\n",
    "    labs = []\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for i, (bg, labels) in enumerate(train_loader):\n",
    "        #labels = labels.to_device()\n",
    "        logits = net(bg, bg.ndata['feats'])\n",
    "        # we save the logits for visualization later\n",
    "        train_logits.append(logits.detach().numpy())\n",
    "        epoch_logits.append(logits.detach().numpy())\n",
    "        labs.append(labels.unsqueeze(1).detach().numpy())\n",
    "\n",
    "        logp = F.log_softmax(logits, 1)\n",
    "        loss = loss_fn(logp, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        \n",
    "    # Calculate accuracy metrics\n",
    "    epoch_logits = np.vstack(epoch_logits)\n",
    "    labs = np.vstack(labs)\n",
    "    \n",
    "    #print(np.argmax(epoch_logits, axis=1).sum())\n",
    "    \n",
    "    f1 = f1_score(labs, np.argmax(np.vstack(epoch_logits), axis=1), average='weighted')\n",
    "    precision = precision_score(labs, np.argmax(np.vstack(epoch_logits), axis=1), average='weighted')\n",
    "    recall = recall_score(labs, np.argmax(np.vstack(epoch_logits), axis=1), average='weighted')\n",
    "    \n",
    "    \n",
    "    epoch_loss /= (i+1)\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch %d | Loss: %.4f | F1: %.4f | Precision: %.4f | Recall: %.4f' % (epoch, epoch_loss, f1, precision, recall))\n",
    "        \n",
    "    epoch_losses.append(epoch_loss)\n",
    "    epoch_f1_scores.append(f1)\n",
    "    epoch_precision_scores.append(precision)\n",
    "    epoch_recall_scores.append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we derive the class weights used above\n",
    "print(sum(labs))\n",
    "print(len(labs))\n",
    "\n",
    "74072-10833\n",
    "63239/10833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(epoch_losses, label=\"Loss\")\n",
    "plt.plot(epoch_f1_scores, label='F1')\n",
    "plt.plot(epoch_precision_scores, label=\"Precision\")\n",
    "plt.plot(epoch_recall_scores, label=\"Recall\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "\n",
    "# Training loop\n",
    "gcn_net.train()\n",
    "epoch_losses = []\n",
    "\n",
    "epoch_f1_scores = [] \n",
    "epoch_precision_scores = []\n",
    "epoch_recall_scores = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    preds = []\n",
    "    labs = []\n",
    "    # Train on batch\n",
    "    for i, (bg, labels) in enumerate(train_loader):\n",
    "        labels = labels.to(device)\n",
    "        graph_feats = bg.ndata.pop('h').to(device)\n",
    "        graph_feats, labels = graph_feats.to(device), labels.to(device)\n",
    "        y_pred = gcn_net(bg, graph_feats)\n",
    "        \n",
    "        preds.append(y_pred.detach().numpy())\n",
    "        labs.append(labels.detach().numpy())\n",
    "\n",
    "        labels = np.argmax(labels, axis=1)\n",
    "        \n",
    "        loss = loss_fn(y_pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        \n",
    "    epoch_loss /= (i + 1)\n",
    "    \n",
    "    preds = np.vstack(preds)\n",
    "    labs = np.vstack(labs)\n",
    "    \n",
    "    # There's some sort of issue going on here with the scoring. All three values are the same\n",
    "    f1 = f1_score(np.argmax(labs, axis=1), np.argmax(preds, axis=1), average='micro')\n",
    "    precision = precision_score(np.argmax(labs, axis=1), np.argmax(preds, axis=1), average='micro')\n",
    "    recall = recall_score(np.argmax(labs, axis=1), np.argmax(preds, axis=1), average='micro')\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"epoch: {epoch}, LOSS: {epoch_loss:.3f}, F1: {f1:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}\")\n",
    "        \n",
    "    epoch_losses.append(epoch_loss)\n",
    "    epoch_f1_scores.append(f1)\n",
    "    epoch_precision_scores.append(precision)\n",
    "    epoch_recall_scores.append(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss =0\n",
    "test_logits = []\n",
    "preds = []\n",
    "labs = []\n",
    "\n",
    "net.eval()\n",
    "for i, (bg, labels) in enumerate(test_loader):\n",
    "    #labels = labels.to(device)\n",
    "    logits = net(bg, bg.ndata['feats'])\n",
    "    \n",
    "    test_logits.append(logits.detach().numpy())\n",
    "    labs.append(labels.unsqueeze(1).detach().numpy())\n",
    "\n",
    "    #logp = F.log_softmax(logits, 1)\n",
    "    \n",
    "test_logits = np.vstack(test_logits)\n",
    "labs = np.vstack(labs)\n",
    "\n",
    "f1 = f1_score(labs, np.argmax(np.vstack(test_logits), axis=1), average='weighted')\n",
    "precision = precision_score(labs, np.argmax(np.vstack(test_logits), axis=1), average='weighted')\n",
    "recall = recall_score(labs, np.argmax(np.vstack(test_logits), axis=1), average='weighted')\n",
    "\n",
    "print('Test: F1: %.4f | Precision: %.4f | Recall: %.4f' % (f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "gcn_net.eval()\n",
    "test_loss = 0\n",
    "\n",
    "preds = []\n",
    "labs = []\n",
    "for i, (bg, labels) in enumerate(test_loader):\n",
    "    labels = labels.to(device)\n",
    "    graph_feats = bg.ndata.pop('h').to(device)\n",
    "    graph_feats, labels = graph_feats.to(device), labels.to(device)\n",
    "    y_pred = gcn_net(bg, graph_feats)\n",
    "\n",
    "    preds.append(y_pred.detach().numpy())\n",
    "    labs.append(labels.detach().numpy())\n",
    "\n",
    "labs = np.vstack(labs)\n",
    "preds = np.vstack(preds)\n",
    "\n",
    "f1 = f1_score(np.argmax(labs, axis=1), np.argmax(preds, axis=1), average='micro')\n",
    "precision = precision_score(np.argmax(labs, axis=1), np.argmax(preds, axis=1), average='micro')\n",
    "recall = recall_score(np.argmax(labs, axis=1), np.argmax(preds, axis=1), average='micro')\n",
    "\n",
    "print(f\"TEST F1: {f1:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
